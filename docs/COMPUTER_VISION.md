# Computer Vision & AI Detection

Documentation for the experimental local AI detection system in Trigger Warnings.

---

## âš ï¸ Experimental Feature

**Status**: ðŸ§ª **Experimental** (disabled by default)

The AI detection system is still in development. It may:
- Miss triggers (false negatives)
- Report false positives
- Impact browser performance
- Require significant RAM (100-200MB extra)

**Use at your own discretion.**

---

## ðŸ“‹ Overview

The Trigger Warnings extension includes **optional local AI models** that can automatically detect triggers in video content without relying on community submissions. This is especially useful for:
- New or obscure content without warnings yet
- Real-time detection during live streams
- Enhanced coverage alongside community warnings

### Key Principles

1. **Privacy-first**: All processing happens in your browser (no video data sent externally)
2. **Multi-modal**: Combines audio, visual, and subtitle analysis
3. **Efficient**: Cascade architecture (fast filters â†’ deep analysis only when needed)
4. **Transparent**: Open-source models with clear limitations

---

## ðŸ—ï¸ Architecture

### Cascade Detection Pipeline

```
Video Frame (60 FPS)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Frame Sampler (2 FPS)            â”‚  â† Reduce load
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CLIP Filter (Fast)               â”‚  â† 50ms per frame
â”‚  Zero-shot image classification   â”‚
â”‚  Threshold: 0.25                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“ (Only if > 0.25)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  YOLO Detector (Medium)           â”‚  â† 100ms per frame
â”‚  Object detection (spiders, etc.) â”‚
â”‚  Threshold: 0.5                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“ (Only if > 0.5)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  VLM Confirmation (Slow)          â”‚  â† 500ms per frame
â”‚  Vision-language model            â”‚
â”‚  Threshold: 0.6                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ TRIGGER! â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Why cascade?**
- Most frames (99%+) are benign â†’ CLIP filter rejects them quickly
- Only suspicious frames go to deeper analysis
- Saves CPU/GPU time and battery

### Audio Analysis (Parallel)

```
Audio Stream
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Audio Sampler (2-second chunks)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CLAP (Audio CLIP)                â”‚  â† 200ms per chunk
â”‚  Zero-shot audio classification   â”‚
â”‚  Detects: screams, retching, etc. â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“ (If match)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Spectrogram Analysis             â”‚  â† 100ms
â”‚  Frequency pattern matching       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ TRIGGER! â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Subtitle Analysis (Parallel)

```
Subtitle Track
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Text Extraction                  â”‚
â”‚  From WebVTT, SRT, or DOM         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Keyword Matching                 â”‚  â† 1ms per subtitle
â”‚  Regex for trigger words          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“ (If keywords found)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Context Analysis (NLP)           â”‚  â† 50ms per subtitle
â”‚  BERT model for false positives   â”‚
â”‚  (e.g., "violence in the news")   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ TRIGGER! â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ§  AI Models

### Visual Analysis

#### CLIP (OpenAI)
- **Purpose**: Fast zero-shot image classification
- **Size**: ~150MB (loaded on demand)
- **Speed**: ~50ms per frame (GPU), ~200ms (CPU)
- **Accuracy**: 70-80% recall, 60-70% precision

**Prompts:**
```typescript
const prompts = {
  violence: "a violent scene with fighting or physical harm",
  gore: "graphic blood, injury, or gore",
  spiders: "a spider or arachnid",
  medical: "a medical procedure, surgery, or needles",
  // ... more categories
};
```

**Usage:**
```typescript
import { Transformers } from '@xenova/transformers';

const clip = await Transformers.CLIPModel.from_pretrained('openai/clip-vit-base-patch32');

const scores = await clip.classify(imageData, {
  candidate_labels: Object.values(prompts),
});

if (scores.violence > 0.25) {
  // Potential violence detected, pass to YOLO
}
```

#### YOLO (You Only Look Once)
- **Purpose**: Object detection (weapons, spiders, etc.)
- **Size**: ~40MB
- **Speed**: ~100ms per frame (GPU), ~500ms (CPU)
- **Accuracy**: 85-90% for trained objects

**Detectable Objects:**
- Spiders (from COCO dataset)
- Weapons (guns, knives)
- Medical equipment (syringes, scalpels)
- Blood (custom-trained)

**Usage:**
```typescript
import * as tf from '@tensorflow/tfjs';

const yolo = await tf.loadGraphModel('/models/yolo-v5.json');

const detections = await yolo.detect(imageData);

for (const detection of detections) {
  if (detection.class === 'spider' && detection.score > 0.5) {
    // Spider detected!
  }
}
```

#### VLM (Vision-Language Model)
- **Purpose**: Context-aware confirmation (reduce false positives)
- **Model**: LLaVA or MiniGPT-4 (via Transformers.js)
- **Size**: ~200MB
- **Speed**: ~500ms per frame (GPU), ~2s (CPU)
- **Accuracy**: 90-95% precision

**Example:**
```typescript
const vlm = await Transformers.VisionEncoderDecoderModel.from_pretrained('llava-hf/llava-1.5-7b');

const prompt = "Is there violence or physical harm in this image? Answer yes or no.";
const response = await vlm.generate(imageData, prompt);

if (response.toLowerCase().includes('yes')) {
  // Confirmed trigger
}
```

### Audio Analysis

#### CLAP (Contrastive Language-Audio Pretraining)
- **Purpose**: Detect auditory triggers (screams, retching, gunshots)
- **Size**: ~100MB
- **Speed**: ~200ms per 2-second chunk
- **Accuracy**: 75-85%

**Prompts:**
```typescript
const audioPrompts = {
  emetophobia: "the sound of someone vomiting or retching",
  gunshots: "loud gunshots or explosions",
  screaming: "someone screaming or crying out in pain",
};
```

**Usage:**
```typescript
const clap = await Transformers.CLAPModel.from_pretrained('laion/clap-htsat-unfused');

const scores = await clap.classify(audioBuffer, {
  candidate_labels: Object.values(audioPrompts),
});

if (scores.emetophobia > 0.3) {
  // Potential emetophobia trigger
}
```

### Subtitle Analysis

#### BERT (NLP)
- **Purpose**: Context-aware keyword detection
- **Model**: DistilBERT (optimized for speed)
- **Size**: ~50MB
- **Speed**: ~50ms per subtitle
- **Accuracy**: 80-90%

**Example:**
```typescript
const bert = await Transformers.AutoModelForSequenceClassification.from_pretrained(
  'distilbert-base-uncased-finetuned-sst-2-english'
);

const subtitleText = "There was violence in the news today"; // Not a trigger
const score = await bert.classify(subtitleText, ['trigger_violence', 'not_trigger']);

if (score.trigger_violence > 0.7) {
  // Likely a depiction, not just discussion
}
```

---

## âš™ï¸ Configuration

### Enabling AI Detection

**In Options page:**
1. Settings â†’ "Experimental Features"
2. Toggle "Local AI Detection"
3. Choose models to enable:
   - â˜‘ï¸ Visual (CLIP + YOLO + VLM)
   - â˜‘ï¸ Audio (CLAP)
   - â˜‘ï¸ Subtitles (BERT)

**Programmatically:**
```typescript
import { DetectionSettings } from '@core/storage';

await DetectionSettings.set({
  enabled: true,
  visual: { enabled: true, cascade: true },
  audio: { enabled: true },
  subtitles: { enabled: true },
});
```

### Performance Settings

**GPU Acceleration:**
```typescript
import '@tensorflow/tfjs-backend-webgpu';

await tf.setBackend('webgpu'); // Fastest (if supported)
// or
await tf.setBackend('webgl'); // Fast (widely supported)
// or
await tf.setBackend('cpu'); // Slow but universal
```

**Memory Management:**
```typescript
// Limit memory usage
tf.env().set('WEBGL_DELETE_TEXTURE_THRESHOLD', 0);

// Clean up models when done
model.dispose();
```

**Sampling Rate:**
```typescript
// Analyze fewer frames (less accurate, faster)
const settings = {
  visual: { fps: 1 }, // Default: 2 FPS
  audio: { chunkSize: 4 }, // Default: 2 seconds
};
```

---

## ðŸ“Š Accuracy & Limitations

### Strengths

âœ… **High recall for obvious triggers**: Detects 80-90% of clear violence, gore, spiders  
âœ… **Fast on GPU**: Real-time on modern hardware (RTX 3060+)  
âœ… **Privacy-preserving**: No data leaves your device  
âœ… **Complements community warnings**: Catches content without submissions  

### Limitations

âŒ **Context-dependent**: May flag action movie previews, medical dramas  
âŒ **False negatives**: Misses ~10-20% of triggers (especially implied violence)  
âŒ **False positives**: ~5-15% of alerts are incorrect  
âŒ **Resource-intensive**: 100-200MB RAM, 5-10% CPU on GPU  
âŒ **Not real-time on CPU**: 2-5 second delay on slower hardware  

### Known Issues

| Issue | Workaround |
|-------|------------|
| High CPU usage | Enable GPU acceleration (WebGPU) |
| False positives on cartoons | Increase VLM threshold (0.6 â†’ 0.8) |
| Misses subtle triggers | Use community warnings as primary |
| Lag on older hardware | Disable AI detection, use community only |

---

## ðŸ§ª Testing & Validation

### Test Content

We validate models against a labeled test set:
- **100 clips** with known triggers (violence, gore, spiders, etc.)
- **100 clips** without triggers (control)

**Metrics:**
- **Recall**: % of triggers detected (aim: >80%)
- **Precision**: % of alerts that are correct (aim: >70%)
- **F1 Score**: Harmonic mean of precision & recall

### Running Tests

```bash
# Run AI detection tests
npm run test:ai

# Generate accuracy report
npm run test:ai:report
```

**Example output:**
```
Visual Detection (CLIP + YOLO + VLM):
  Violence: Recall 85%, Precision 72%, F1 78%
  Gore: Recall 78%, Precision 68%, F1 73%
  Spiders: Recall 92%, Precision 88%, F1 90%

Audio Detection (CLAP):
  Emetophobia: Recall 80%, Precision 65%, F1 72%
  Gunshots: Recall 88%, Precision 82%, F1 85%

Overall: Recall 83%, Precision 73%, F1 77%
```

---

## ðŸ› ï¸ Development

### Adding a New Trigger Type

1. **Add to prompts** (`src/_Detection_System/config.ts`):
   ```typescript
   export const VISUAL_PROMPTS = {
     // ... existing
     my_new_trigger: "description of visual trigger",
   };
   
   export const AUDIO_PROMPTS = {
     // ... existing
     my_new_trigger: "description of auditory trigger",
   };
   ```

2. **Update type definitions** (`src/shared/types/Detection.types.ts`):
   ```typescript
   export type DetectionTriggerType = 
     | 'violence'
     | 'gore'
     | 'my_new_trigger'; // Add here
   ```

3. **Test on labeled data**:
   - Collect 50 clips with the trigger
   - Collect 50 clips without it
   - Run `npm run test:ai:single my_new_trigger`
   - Aim for F1 > 70%

4. **Submit PR** with test results

### Training Custom Models

**For advanced users** who want to fine-tune models:

1. **Collect training data**: 1000+ labeled images/audio clips
2. **Fine-tune CLIP**:
   ```python
   from transformers import CLIPProcessor, CLIPModel, Trainer

   model = CLIPModel.from_pretrained('openai/clip-vit-base-patch32')
   # ... fine-tuning code ...
   ```

3. **Convert to TFJS**:
   ```bash
   tensorflowjs_converter --input_format=tf_saved_model \
     --output_format=tfjs_graph_model \
     /path/to/saved_model \
     /path/to/output
   ```

4. **Test in extension**:
   ```typescript
   const customModel = await tf.loadGraphModel('/custom-models/my-model.json');
   ```

---

## ðŸ”® Future Improvements

- **More models**: Emotion recognition, object segmentation
- **Federated learning**: Improve models based on user feedback (privacy-preserving)
- **Lighter models**: <50MB for mobile browsers (TinyML)
- **Real-time on CPU**: Optimize for faster inference
- **Custom training**: Let users train on their own labeled data

---

## ðŸ“š References

- **CLIP Paper**: https://arxiv.org/abs/2103.00020
- **YOLO Paper**: https://arxiv.org/abs/1506.02640
- **CLAP Paper**: https://arxiv.org/abs/2211.06687
- **Transformers.js**: https://huggingface.co/docs/transformers.js
- **TensorFlow.js**: https://www.tensorflow.org/js

---

<div align="center">

**Questions?** [Open a discussion](https://github.com/mitchlabeetch/Trigger_Warnings/discussions) ðŸ’¬

[Back to README](../README.md)

</div>
