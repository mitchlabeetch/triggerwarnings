/**
 * ALGORITHM 3.0 - PHASE 9: INNOVATION #32
 * Meta-Learner (MAML-inspired)
 *
 * Model-Agnostic Meta-Learning for rapid adaptation to new trigger patterns
 * with just 1-5 examples. Learns optimal initialization parameters that enable
 * fast fine-tuning to novel categories.
 *
 * Research: Finn et al. (2017) - Model-Agnostic Meta-Learning for Fast Adaptation
 *           +15-20% accuracy on novel patterns with few examples
 *
 * Equal Treatment: All 28 categories benefit from same meta-learning approach
 */
import { logger } from '../utils/Logger';
/**
 * Meta-Learner
 *
 * Learns optimal initialization for rapid adaptation to new patterns
 */
export class MetaLearner {
    // Meta-learned initialization parameters
    metaParameters;
    // Task-specific adapted parameters (cache)
    adaptedParameters = new Map();
    // Meta-learning hyperparameters
    INNER_LR = 0.01; // Inner loop learning rate (task adaptation)
    OUTER_LR = 0.001; // Outer loop learning rate (meta-update)
    MAX_INNER_STEPS = 5; // Max gradient steps for adaptation
    CONVERGENCE_THRESHOLD = 0.001;
    // Model architecture
    INPUT_DIM = 256;
    HIDDEN_DIM = 128;
    OUTPUT_DIM = 1;
    // Statistics
    stats = {
        totalAdaptations: 0,
        totalMetaTasks: 0,
        avgAdaptationGain: 0,
        avgGradientSteps: 0,
        avgFinalLoss: 0,
        convergenceRate: 0,
        fewShotAccuracy: new Map()
    };
    constructor() {
        logger.info('[MetaLearner] ðŸ§  Meta-Learner initialized (MAML-inspired)');
        logger.info('[MetaLearner] ðŸš€ Learning optimal init for rapid 1-5 shot adaptation');
        // Initialize meta-parameters
        this.metaParameters = this.initializeParameters();
    }
    // ========================================
    // META-LEARNING
    // ========================================
    /**
     * Adapt to new task using support set (1-5 shot learning)
     */
    adapt(task) {
        this.stats.totalAdaptations++;
        // Step 1: Start from meta-learned initialization
        let adaptedParams = this.cloneParameters(this.metaParameters);
        // Step 2: Compute pre-adaptation confidence
        const preAdaptationConf = this.predict(task.querySet[0]?.features || [], this.metaParameters);
        // Step 3: Inner loop - adapt to support set
        let numSteps = 0;
        let finalLoss = 0;
        let converged = false;
        for (let step = 0; step < this.MAX_INNER_STEPS; step++) {
            // Compute loss on support set
            const loss = this.computeLoss(task.supportSet, adaptedParams);
            // Check convergence
            if (step > 0 && Math.abs(finalLoss - loss) < this.CONVERGENCE_THRESHOLD) {
                converged = true;
                break;
            }
            finalLoss = loss;
            // Compute gradients
            const gradients = this.computeGradients(task.supportSet, adaptedParams);
            // Update parameters (gradient descent)
            adaptedParams = this.updateParameters(adaptedParams, gradients, this.INNER_LR);
            numSteps++;
        }
        // Step 4: Compute post-adaptation confidence
        const adaptedConf = this.predict(task.querySet[0]?.features || [], adaptedParams);
        // Step 5: Cache adapted parameters
        this.adaptedParameters.set(task.category, adaptedParams);
        // Step 6: Compute adaptation gain
        const adaptationGain = adaptedConf - preAdaptationConf;
        // Update statistics
        this.updateStats(adaptationGain, numSteps, finalLoss, converged, task.supportSet.length);
        logger.debug(`[MetaLearner] ${task.category}: adapted in ${numSteps} steps, ` +
            `gain=+${(adaptationGain * 100).toFixed(1)}%, loss=${finalLoss.toFixed(4)}`);
        return {
            category: task.category,
            adaptedConfidence: adaptedConf,
            preAdaptationConfidence: preAdaptationConf,
            adaptationGain,
            numGradientSteps: numSteps,
            finalLoss,
            converged
        };
    }
    /**
     * Meta-train on multiple tasks (outer loop)
     */
    metaTrain(tasks) {
        this.stats.totalMetaTasks += tasks.length;
        // For each task, compute meta-gradient
        const metaGradients = [];
        for (const task of tasks) {
            // Step 1: Adapt to task (inner loop)
            const adaptedParams = this.innerLoop(task.supportSet);
            // Step 2: Compute loss on query set with adapted parameters
            const queryLoss = this.computeLoss(task.querySet, adaptedParams);
            // Step 3: Compute meta-gradient (gradient of query loss w.r.t. meta-params)
            const metaGrad = this.computeMetaGradient(task.supportSet, task.querySet, queryLoss);
            metaGradients.push(metaGrad);
        }
        // Step 4: Average meta-gradients across tasks
        const avgMetaGrad = this.averageGradients(metaGradients);
        // Step 5: Update meta-parameters (outer loop)
        this.metaParameters = this.updateParameters(this.metaParameters, avgMetaGrad, this.OUTER_LR);
        logger.info(`[MetaLearner] ðŸ“š Meta-trained on ${tasks.length} tasks, ` +
            `updated meta-initialization`);
    }
    /**
     * Inner loop adaptation (for meta-training)
     */
    innerLoop(supportSet) {
        let params = this.cloneParameters(this.metaParameters);
        for (let step = 0; step < this.MAX_INNER_STEPS; step++) {
            const gradients = this.computeGradients(supportSet, params);
            params = this.updateParameters(params, gradients, this.INNER_LR);
        }
        return params;
    }
    // ========================================
    // PREDICTION
    // ========================================
    /**
     * Predict using parameters
     */
    predict(features, params) {
        // Simple feedforward: features -> hidden -> output
        const hidden = this.forward(features, params.weights, params.bias);
        return this.sigmoid(hidden[0] || 0);
    }
    /**
     * Predict with adapted parameters (if available)
     */
    predictAdapted(features, category) {
        const adaptedParams = this.adaptedParameters.get(category);
        if (adaptedParams) {
            return this.predict(features, adaptedParams);
        }
        // Fallback to meta-parameters
        return this.predict(features, this.metaParameters);
    }
    /**
     * Forward pass
     */
    forward(input, weights, bias) {
        const output = [];
        for (let i = 0; i < weights.length; i++) {
            let sum = bias[i] || 0;
            for (let j = 0; j < Math.min(input.length, weights[i].length); j++) {
                sum += input[j] * weights[i][j];
            }
            output.push(sum);
        }
        return output;
    }
    // ========================================
    // LOSS & GRADIENTS
    // ========================================
    /**
     * Compute loss on examples
     */
    computeLoss(examples, params) {
        let totalLoss = 0;
        for (const example of examples) {
            const pred = this.predict(example.features, params);
            const target = example.label ? 1 : 0;
            // Binary cross-entropy loss
            totalLoss += -target * Math.log(pred + 1e-8) - (1 - target) * Math.log(1 - pred + 1e-8);
        }
        return examples.length > 0 ? totalLoss / examples.length : 0;
    }
    /**
     * Compute gradients (simplified - numerical gradients)
     */
    computeGradients(examples, params) {
        const epsilon = 1e-4;
        const gradWeights = [];
        const gradBias = [];
        // Numerical gradients for weights
        for (let i = 0; i < params.weights.length; i++) {
            const rowGrads = [];
            for (let j = 0; j < params.weights[i].length; j++) {
                // Compute finite difference
                params.weights[i][j] += epsilon;
                const lossPlus = this.computeLoss(examples, params);
                params.weights[i][j] -= 2 * epsilon;
                const lossMinus = this.computeLoss(examples, params);
                params.weights[i][j] += epsilon; // Restore
                const grad = (lossPlus - lossMinus) / (2 * epsilon);
                rowGrads.push(grad);
            }
            gradWeights.push(rowGrads);
        }
        // Numerical gradients for bias
        for (let i = 0; i < params.bias.length; i++) {
            params.bias[i] += epsilon;
            const lossPlus = this.computeLoss(examples, params);
            params.bias[i] -= 2 * epsilon;
            const lossMinus = this.computeLoss(examples, params);
            params.bias[i] += epsilon; // Restore
            const grad = (lossPlus - lossMinus) / (2 * epsilon);
            gradBias.push(grad);
        }
        return { weights: gradWeights, bias: gradBias };
    }
    /**
     * Compute meta-gradient (gradient of query loss w.r.t. meta-params)
     */
    computeMetaGradient(supportSet, querySet, queryLoss) {
        // Simplified: approximate with query set gradients
        const adaptedParams = this.innerLoop(supportSet);
        return this.computeGradients(querySet, adaptedParams);
    }
    // ========================================
    // PARAMETER UPDATES
    // ========================================
    /**
     * Update parameters using gradients
     */
    updateParameters(params, gradients, learningRate) {
        const newWeights = [];
        const newBias = [];
        // Update weights
        for (let i = 0; i < params.weights.length; i++) {
            const row = [];
            for (let j = 0; j < params.weights[i].length; j++) {
                row.push(params.weights[i][j] - learningRate * (gradients.weights[i]?.[j] || 0));
            }
            newWeights.push(row);
        }
        // Update bias
        for (let i = 0; i < params.bias.length; i++) {
            newBias.push(params.bias[i] - learningRate * (gradients.bias[i] || 0));
        }
        return { weights: newWeights, bias: newBias };
    }
    /**
     * Average gradients across tasks
     */
    averageGradients(gradients) {
        if (gradients.length === 0) {
            return this.initializeParameters();
        }
        const avgWeights = [];
        const avgBias = [];
        // Average weights
        const numRows = gradients[0].weights.length;
        const numCols = gradients[0].weights[0].length;
        for (let i = 0; i < numRows; i++) {
            const row = [];
            for (let j = 0; j < numCols; j++) {
                let sum = 0;
                for (const grad of gradients) {
                    sum += grad.weights[i]?.[j] || 0;
                }
                row.push(sum / gradients.length);
            }
            avgWeights.push(row);
        }
        // Average bias
        const biasLen = gradients[0].bias.length;
        for (let i = 0; i < biasLen; i++) {
            let sum = 0;
            for (const grad of gradients) {
                sum += grad.bias[i] || 0;
            }
            avgBias.push(sum / gradients.length);
        }
        return { weights: avgWeights, bias: avgBias };
    }
    // ========================================
    // UTILITIES
    // ========================================
    /**
     * Initialize parameters with Xavier initialization
     */
    initializeParameters() {
        const scale = Math.sqrt(2.0 / (this.INPUT_DIM + this.HIDDEN_DIM));
        const weights = [];
        for (let i = 0; i < this.HIDDEN_DIM; i++) {
            const row = [];
            for (let j = 0; j < this.INPUT_DIM; j++) {
                row.push((Math.random() - 0.5) * 2 * scale);
            }
            weights.push(row);
        }
        const bias = new Array(this.HIDDEN_DIM).fill(0);
        return { weights, bias };
    }
    /**
     * Clone parameters
     */
    cloneParameters(params) {
        return {
            weights: params.weights.map(row => [...row]),
            bias: [...params.bias]
        };
    }
    /**
     * Sigmoid activation
     */
    sigmoid(x) {
        return 1 / (1 + Math.exp(-x));
    }
    // ========================================
    // STATISTICS
    // ========================================
    /**
     * Update statistics
     */
    updateStats(gain, steps, loss, converged, numShots) {
        const total = this.stats.totalAdaptations;
        this.stats.avgAdaptationGain =
            (this.stats.avgAdaptationGain * (total - 1) + gain) / total;
        this.stats.avgGradientSteps =
            (this.stats.avgGradientSteps * (total - 1) + steps) / total;
        this.stats.avgFinalLoss =
            (this.stats.avgFinalLoss * (total - 1) + loss) / total;
        if (converged) {
            this.stats.convergenceRate =
                (this.stats.convergenceRate * (total - 1) + 1) / total;
        }
        else {
            this.stats.convergenceRate =
                (this.stats.convergenceRate * (total - 1)) / total;
        }
        // Track few-shot accuracy by number of shots
        const currentAcc = this.stats.fewShotAccuracy.get(numShots) || 0;
        const currentCount = this.stats.fewShotAccuracy.get(numShots) || 1;
        this.stats.fewShotAccuracy.set(numShots, (currentAcc * currentCount + (gain > 0 ? 1 : 0)) / (currentCount + 1));
    }
    /**
     * Get statistics
     */
    getStats() {
        return {
            ...this.stats,
            fewShotAccuracy: new Map(this.stats.fewShotAccuracy)
        };
    }
    /**
     * Clear state
     */
    clear() {
        this.metaParameters = this.initializeParameters();
        this.adaptedParameters.clear();
        this.stats = {
            totalAdaptations: 0,
            totalMetaTasks: 0,
            avgAdaptationGain: 0,
            avgGradientSteps: 0,
            avgFinalLoss: 0,
            convergenceRate: 0,
            fewShotAccuracy: new Map()
        };
        logger.info('[MetaLearner] ðŸ§¹ Cleared meta-learner state');
    }
}
// Singleton instance
export const metaLearner = new MetaLearner();
//# sourceMappingURL=MetaLearner.js.map